{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea42d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import *\n",
    "\n",
    "from evalutils import get_feature_names, evaluate_ner\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Pretty plots\n",
    "%matplotlib inline\n",
    "sns.set_style('ticks')\n",
    "plt.rcParams['figure.figsize'] = (6, 4)\n",
    "plt.rcParams['axes.titlesize'] = 22\n",
    "plt.rcParams['axes.labelsize'] = 20\n",
    "plt.rcParams['xtick.labelsize'] = 16\n",
    "plt.rcParams['ytick.labelsize'] = 16\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "plt.rcParams['legend.title_fontsize'] = 12\n",
    "\n",
    "# Display wide columns\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fee221",
   "metadata": {},
   "source": [
    "___\n",
    "# Gold standard annotations\n",
    "### Load gold standard concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cffdcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1137, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>histopathology_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>report_no</th>\n",
       "      <th>concept</th>\n",
       "      <th>phrase</th>\n",
       "      <th>position</th>\n",
       "      <th>start_char</th>\n",
       "      <th>end_char</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>754</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>SampleType</td>\n",
       "      <td>skin</td>\n",
       "      <td>202 206</td>\n",
       "      <td>202</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>214</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>FungalDescriptor</td>\n",
       "      <td>Fungal elements</td>\n",
       "      <td>1374 1389</td>\n",
       "      <td>1374</td>\n",
       "      <td>1389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Stain</td>\n",
       "      <td>Grocott</td>\n",
       "      <td>1412 1419</td>\n",
       "      <td>1412</td>\n",
       "      <td>1419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>214</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>identified</td>\n",
       "      <td>1394 1404</td>\n",
       "      <td>1394</td>\n",
       "      <td>1404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>214</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>FungalDescriptor</td>\n",
       "      <td>Fungal elements</td>\n",
       "      <td>1593 1608</td>\n",
       "      <td>1593</td>\n",
       "      <td>1608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   histopathology_id  patient_id  report_no           concept  \\\n",
       "0                754           1          1        SampleType   \n",
       "1                214           2          1  FungalDescriptor   \n",
       "2                214           2          1             Stain   \n",
       "3                214           2          1          positive   \n",
       "4                214           2          1  FungalDescriptor   \n",
       "\n",
       "            phrase   position  start_char  end_char  \n",
       "0             skin    202 206         202       206  \n",
       "1  Fungal elements  1374 1389        1374      1389  \n",
       "2          Grocott  1412 1419        1412      1419  \n",
       "3       identified  1394 1404        1394      1404  \n",
       "4  Fungal elements  1593 1608        1593      1608  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load gold standard concepts\n",
    "true_concepts = pd.read_csv(\"gold_concepts.csv\")\n",
    "true_concepts.drop(['concept_id','preceding', 'following'], axis=1, inplace=True)\n",
    "print(true_concepts.shape)\n",
    "true_concepts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2360cbd",
   "metadata": {},
   "source": [
    "### Separate discontinuous concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f49b1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discont concepts have ;-separated positions\n",
    "idx = true_concepts[true_concepts.position.str.contains(\";\")].index\n",
    "\n",
    "# Split discont concepts into a separate dataframe\n",
    "discont = true_concepts.iloc[idx].copy()\n",
    "true_concepts.drop(idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3486a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1155, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loop over discont concepts extracting individual spans\n",
    "for _,x in discont.iterrows():\n",
    "    spans = []\n",
    "    i = 0\n",
    "    for pos in x.position.split(';'):\n",
    "        # Extract start and end char positions\n",
    "        start_char, end_char = map(int, pos.split())\n",
    "        # Calculate span length\n",
    "        len_span = end_char - start_char\n",
    "        # Extract span text\n",
    "        phrase = x.phrase[i:i+len_span]\n",
    "        # Add to list of spans\n",
    "        spans.append((start_char, end_char, phrase))\n",
    "        i = i + len_span + 1\n",
    "        \n",
    "    # Sort extracted spans by starting position\n",
    "    spans = sorted(spans, key=lambda x: x[0])\n",
    "    \n",
    "    # Append extracted spans to the dataframe with gold standard concepts \n",
    "    for span in spans:\n",
    "        tmp = x.copy()\n",
    "        tmp['start_char'] = span[0]\n",
    "        tmp['end_char'] = span[1]\n",
    "        tmp['phrase'] = span[2]\n",
    "        true_concepts = pd.concat([true_concepts, tmp.to_frame().T], axis=0, ignore_index=True)\n",
    "        \n",
    "# Remove position column\n",
    "true_concepts.drop('position', axis=1, inplace=True)\n",
    "true_concepts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd95e87",
   "metadata": {},
   "source": [
    "### Load tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69943485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy model with disable NER\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=['ner'])\n",
    "\n",
    "# Apply tokeniser to gold standard annotations\n",
    "true_concepts['doc'] = true_concepts.phrase.apply(nlp.tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e7075",
   "metadata": {},
   "source": [
    "### Assign BIOES tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69eb992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe to store concepts with BIOES tags\n",
    "true_concepts_bioes = pd.DataFrame(columns=true_concepts.columns)\n",
    "\n",
    "# Single-token entities\n",
    "true_concepts_bioes = true_concepts[true_concepts.doc.apply(len) == 1].copy()\n",
    "\n",
    "# Add the \"S\" tag\n",
    "true_concepts_bioes.concept = true_concepts_bioes.concept.apply(lambda x: \"S-\" + x)\n",
    "\n",
    "# Remove doc\n",
    "true_concepts_bioes.drop('doc', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34f14640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1649, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multi-token entities\n",
    "for _,x in true_concepts[true_concepts.doc.apply(len) > 1].iterrows():\n",
    "    \n",
    "    # Loop over tokens\n",
    "    for token in x.doc:\n",
    "        \n",
    "        # Skip if whitespace\n",
    "        if token.is_space:\n",
    "            continue\n",
    "        \n",
    "        # If the first token tag with \"B-\"\n",
    "        if token.i==0:\n",
    "            concept = \"B-\" + x.concept\n",
    "            \n",
    "        # If the last token tag with \"E-\"\n",
    "        elif token.i+1==len(x.doc):\n",
    "            concept = \"E-\" + x.concept\n",
    "            \n",
    "        # If in the middle tag with \"I-\"\n",
    "        else:\n",
    "            concept = \"I-\" + x.concept\n",
    "\n",
    "        # Adjust start char position\n",
    "        start_char = x.start_char + token.idx \n",
    "\n",
    "        tmp = pd.DataFrame({\n",
    "            'histopathology_id': x.histopathology_id,\n",
    "            'patient_id': x.patient_id, \n",
    "            'report_no': x.report_no, \n",
    "            'concept': concept, \n",
    "            'phrase': token,\n",
    "            'start_char': start_char,\n",
    "            'end_char': start_char + len(token),\n",
    "        }, index=[0])\n",
    "\n",
    "        # Add to the table of concepts\n",
    "        true_concepts_bioes = pd.concat([true_concepts_bioes, tmp], axis=0, ignore_index=True) \n",
    "        \n",
    "# Sort BIOES tagged concepts\n",
    "true_concepts_bioes.sort_values(by=['histopathology_id', 'start_char'], inplace=True)\n",
    "true_concepts_bioes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bd2b12",
   "metadata": {},
   "source": [
    "___\n",
    "# Prepare data\n",
    "### Load reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "201da83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>histopathology_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>report_no</th>\n",
       "      <th>order_results</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>214</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>\"URNO     XXXXXXXX \\nLab No    XXXXXXXXX        Specimen  BRUSHINGS \\n\\n\\nCLINICAL NOTES: \\n\\nLU...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>\"URNO     XXXXXXXXX \\nLab No    XXXXXXXXX        Specimen  BAL \\n\\n\\nSPECIMEN \\n1. Right upper l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>833</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>\"URNO     XXXXXXXXX \\nLab No    XXXXXXXXX        Specimen  WASHINGS \\n\\n\\nCLINICAL NOTES: \\nNo c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>\"XXXXXXX F XXXXXXXXXX  Report (XXXXXXXX)\\nMACROSCOPIC DESCRIPTION:    Biopsy of cyst\"\": Three fr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>649</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>\"XXXXXXX \\nM\\nXXXXXXXXX\\n \\nReport (XXXXXXXX)\\nCLINICAL NOTES:   AML R bronchial thickening on C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   histopathology_id  patient_id  report_no  \\\n",
       "0                214           2          1   \n",
       "1                127          23          1   \n",
       "2                833          25          1   \n",
       "3                194          38          1   \n",
       "4                649          48          1   \n",
       "\n",
       "                                                                                         order_results  \\\n",
       "0  \"URNO     XXXXXXXX \\nLab No    XXXXXXXXX        Specimen  BRUSHINGS \\n\\n\\nCLINICAL NOTES: \\n\\nLU...   \n",
       "1  \"URNO     XXXXXXXXX \\nLab No    XXXXXXXXX        Specimen  BAL \\n\\n\\nSPECIMEN \\n1. Right upper l...   \n",
       "2  \"URNO     XXXXXXXXX \\nLab No    XXXXXXXXX        Specimen  WASHINGS \\n\\n\\nCLINICAL NOTES: \\nNo c...   \n",
       "3  \"XXXXXXX F XXXXXXXXXX  Report (XXXXXXXX)\\nMACROSCOPIC DESCRIPTION:    Biopsy of cyst\"\": Three fr...   \n",
       "4  \"XXXXXXX \\nM\\nXXXXXXXXX\\n \\nReport (XXXXXXXX)\\nCLINICAL NOTES:   AML R bronchial thickening on C...   \n",
       "\n",
       "   y  \n",
       "0  1  \n",
       "1  1  \n",
       "2  1  \n",
       "3  1  \n",
       "4  1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the development set of reports\n",
    "df = pd.read_csv(\"reports_test_justin.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0040de6",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df5fd468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'senter', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load spacy model with disable NER\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=['ner'])\n",
    "\n",
    "# Enable sentensizer\n",
    "nlp.enable_pipe('senter')\n",
    "\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38450244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the NLP pipe   line\n",
    "df['doc'] = df.order_results.apply(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9b35d1",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0ee5a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(doc, i):\n",
    "    \"\"\"\n",
    "    Extract the previous and following tokens ignoring whitespaces.\n",
    "    \"\"\"\n",
    "    if (i==0) or (i==1 and doc[i-1].is_space):\n",
    "        prev_token = \"\"\n",
    "    else:\n",
    "        prev_token = doc[i-2].text if doc[i-1].is_space else doc[i-1].text\n",
    "        \n",
    "    if (i==len(doc)-1) or (i==len(doc)-2 and doc[-1].is_space):\n",
    "        next_token = \"\"\n",
    "    else:\n",
    "        next_token = doc[i+2].text if doc[i+1].is_space else doc[i+1].text\n",
    "    \n",
    "    return prev_token, next_token\n",
    "    \n",
    "    \n",
    "def create_features(doc):\n",
    "    \"\"\"\n",
    "    Parses a doc and creates a dictionary of features for each token that is not a whitespace.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for token in doc:\n",
    "        \n",
    "        # Skip if whitespace\n",
    "        if token.is_space:\n",
    "            continue\n",
    "            \n",
    "        # Get previous and next token\n",
    "        prev_token, next_token = get_neighbors(doc, token.i)\n",
    "\n",
    "        # Create a dict of features\n",
    "        token_features = {\n",
    "                'phrase': token.text,\n",
    "                'start_char': token.idx,\n",
    "                'end_char': token.idx + len(token),\n",
    "                'is_capitilized': token.is_alpha and (token.text[0] == token.text.upper()[0]),\n",
    "                'is_upper': token.is_upper,\n",
    "                'is_lower': token.is_lower,\n",
    "                'prefix1': token.text[:1],\n",
    "                'prefix2': token.text[:2] if len(token)>1 else \"\",\n",
    "                'prefix3': token.text[:3] if len(token)>2 else \"\",\n",
    "                'suffix1': token.text[-1:],\n",
    "                'suffix2': token.text[-2:] if len(token)>1 else \"\",\n",
    "                'suffix3': token.text[-3:] if len(token)>2 else \"\",\n",
    "                'prev_token': prev_token,\n",
    "                'next_token': next_token,\n",
    "                'has_hyphen': '-' in token.text,\n",
    "                'is_alpha': token.is_alpha,\n",
    "                'is_digit': token.is_digit,\n",
    "                'is_sent_start': token.is_sent_start,\n",
    "                'is_sent_end': token.is_sent_end,\n",
    "                'is_punct': token.is_punct,\n",
    "            }\n",
    "        \n",
    "        features.append(token_features)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8e2c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature map\n",
    "df['token_features'] = df.doc.apply(create_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f6697c",
   "metadata": {},
   "source": [
    "### Label concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3557eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(x):\n",
    "    \"\"\"\n",
    "    Assigns categories to gold standard concepts or 0, if a token was not annotated.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for token in x.doc:\n",
    "        # Skip if whitespace\n",
    "        if token.is_space:\n",
    "            continue\n",
    "\n",
    "        # Is there an annotated entity in the same location?\n",
    "        concept = true_concepts_bioes.loc[(true_concepts_bioes.histopathology_id==x.histopathology_id) & \n",
    "                                          (true_concepts_bioes.start_char==token.idx), 'concept'] \n",
    "        \n",
    "        # Assign labels\n",
    "        if concept.empty:\n",
    "            labels.append(\"O\")\n",
    "        else:\n",
    "            labels.append(concept.iloc[0])\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c35fa04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33 s, sys: 552 ms, total: 33.6 s\n",
      "Wall time: 36.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Labels\n",
    "df['token_labels'] = df.apply(create_labels, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eb9e77",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b26d543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With parameters c1=0.01 and c2=0.01, the model achieves 0.50 (+/- 0.10).\n",
      "With parameters c1=0.01 and c2=0.10, the model achieves 0.51 (+/- 0.11).\n",
      "With parameters c1=0.01 and c2=1.00, the model achieves 0.44 (+/- 0.11).\n",
      "With parameters c1=0.01 and c2=10.00, the model achieves 0.19 (+/- 0.04).\n",
      "With parameters c1=0.10 and c2=0.01, the model achieves 0.48 (+/- 0.10).\n",
      "With parameters c1=0.10 and c2=0.10, the model achieves 0.50 (+/- 0.10).\n",
      "With parameters c1=0.10 and c2=1.00, the model achieves 0.44 (+/- 0.11).\n",
      "With parameters c1=0.10 and c2=10.00, the model achieves 0.19 (+/- 0.04).\n",
      "With parameters c1=1.00 and c2=0.01, the model achieves 0.45 (+/- 0.10).\n",
      "With parameters c1=1.00 and c2=0.10, the model achieves 0.45 (+/- 0.10).\n",
      "With parameters c1=1.00 and c2=1.00, the model achieves 0.43 (+/- 0.11).\n",
      "With parameters c1=1.00 and c2=10.00, the model achieves 0.19 (+/- 0.04).\n",
      "With parameters c1=10.00 and c2=0.01, the model achieves 0.22 (+/- 0.05).\n",
      "With parameters c1=10.00 and c2=0.10, the model achieves 0.21 (+/- 0.05).\n",
      "With parameters c1=10.00 and c2=1.00, the model achieves 0.20 (+/- 0.05).\n",
      "With parameters c1=10.00 and c2=10.00, the model achieves 0.14 (+/- 0.03).\n",
      "Best macro F1 score = 0.51. Best hyperparameter values: c1=0.01 and c2=0.10.\n",
      "CPU times: user 12h 40min 3s, sys: 6min 33s, total: 12h 46min 36s\n",
      "Wall time: 13h 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = df.order_results\n",
    "y = df.y\n",
    "groups = df.patient_id\n",
    "\n",
    "param_space = {\n",
    "    'c1': [0.01, 0.1, 1, 10],\n",
    "    'c2': [0.01, 0.1, 1, 10],\n",
    "#     'all_possible_states': [False, True],\n",
    "#     'all_possible_transitions': [False, True],\n",
    "}\n",
    "\n",
    "best_params = None\n",
    "best_score = float('-inf')\n",
    "\n",
    "for c1 in param_space['c1']:\n",
    "    for c2 in param_space['c2']:\n",
    "\n",
    "        cv = StratifiedGroupKFold(n_splits=10, shuffle=True, random_state=3)\n",
    "        f1_score = []\n",
    "\n",
    "        for train_idx, val_idx in cv.split(X, y, groups):\n",
    "            \n",
    "            # Initialise CRF object \n",
    "            crf = CRF(algorithm='lbfgs', c1=c1, c2=c2)\n",
    "\n",
    "            # Train the model\n",
    "            crf.fit(df.loc[train_idx, 'token_features'], df.loc[train_idx, 'token_labels'])\n",
    "\n",
    "            # Make predictions on the validation fold\n",
    "            y_pred = crf.predict(df.loc[val_idx, 'token_features'])\n",
    "\n",
    "            # Calculate macro f1\n",
    "            f1_score.append(flat_f1_score(df.loc[val_idx, 'token_labels'], y_pred, average='macro'))\n",
    "            \n",
    "        print(\"With parameters c1=%.2f and c2=%.2f, the model achieves %.2f (+/- %.2f).\" % \n",
    "              (c1, c2, np.mean(f1_score), np.std(f1_score)))\n",
    "\n",
    "        if np.mean(f1_score) > best_score:\n",
    "            best_score = np.mean(f1_score)\n",
    "            best_params = {'c1': c1, 'c2': c2}\n",
    "                \n",
    "                \n",
    "print(\"Best macro F1 score = %.2f. Best hyperparameter values: c1=%.2f and c2=%.2f.\" % \n",
    "      (best_score, best_params['c1'], best_params['c2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88732d9c",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbf2046",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = df.order_results\n",
    "y = df.y\n",
    "groups = df.patient_id\n",
    "\n",
    "df['y_pred'] = np.empty((len(df), 0)).tolist()\n",
    "\n",
    "cv = StratifiedGroupKFold(n_splits=10, shuffle=True, random_state=3)\n",
    "\n",
    "for train_idx, val_idx in cv.split(X, y, groups):\n",
    "            \n",
    "    # Initialise CRF object \n",
    "    crf = CRF(algorithm='lbfgs', c1=0.01, c2=0.1, max_iterations=100, all_possible_transitions=True)\n",
    "\n",
    "    # Train the model\n",
    "    crf.fit(df.loc[train_idx, 'token_features'], df.loc[train_idx, 'token_labels'])\n",
    "\n",
    "    # Make predictions on the validation fold\n",
    "    df.loc[val_idx, 'y_pred'] = pd.Series(crf.predict(df.loc[val_idx, 'token_features']), \n",
    "                                          index=val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843b766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_concepts_bioes = pd.DataFrame(columns=['histopathology_id', 'patient_id', 'report_no', 'fold',\n",
    "                                                'concept', 'phrase', 'start_char', 'end_char'])\n",
    "\n",
    "for _,x in df.iterrows(): \n",
    "    \n",
    "    # Convert to dataframe\n",
    "    tmp = pd.concat([pd.DataFrame(x.token_features, columns=['phrase', 'start_char', 'end_char']), \n",
    "                     pd.Series(x.y_pred, name='concept')],\n",
    "                    axis=1)\n",
    "    tmp = tmp[tmp.concept!='O']\n",
    "    \n",
    "    # Add metadata\n",
    "    tmp['histopathology_id'] = x.histopathology_id\n",
    "    tmp['patient_id'] = x.patient_id\n",
    "    tmp['report_no'] = x.report_no\n",
    "    tmp['fold'] = x.fold  \n",
    "    \n",
    "    \n",
    "    # Add to the table of detected concepts\n",
    "    detected_concepts_bioes = pd.concat([detected_concepts_bioes, tmp], axis=0, ignore_index=True)   \n",
    "    \n",
    "detected_concepts_bioes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce194e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_concepts = pd.DataFrame(columns=['histopathology_id', 'patient_id', 'report_no', 'fold',\n",
    "                                          'concept', 'phrase', 'start_char', 'end_char'])\n",
    "\n",
    "for _,x in df.iterrows(): \n",
    "    \n",
    "    ents = {k:[] for k in ('concept', 'phrase', 'start_char', 'end_char')}\n",
    "\n",
    "    for i,y in enumerate(x.y_pred):\n",
    "        if y==\"O\":\n",
    "            continue\n",
    "        if y.startswith(\"S-\"):\n",
    "            # Record start and end char positions\n",
    "            start_char = x.token_features[i]['start_char']\n",
    "            end_char = x.token_features[i]['end_char']\n",
    "\n",
    "            # Add single-token entity\n",
    "            ents['concept'].append(y[2:])\n",
    "            ents['phrase'].append(x.order_results[start_char:end_char])\n",
    "            ents['start_char'].append(start_char)\n",
    "            ents['end_char'].append(end_char)\n",
    "\n",
    "            # Reset start_char, end_char (optional)\n",
    "            start_char, end_char = None, None\n",
    "\n",
    "        elif y.startswith(\"B-\"):\n",
    "            # Only track a multi-token entity if B is followed by I or E\n",
    "            if x.y_pred[i+1].startswith(\"I-\") or x.y_pred[i+1].startswith(\"E-\"):\n",
    "                # Record start char position\n",
    "                start_char = x.token_features[i]['start_char']\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        elif y.startswith(\"I-\"):\n",
    "            continue\n",
    "\n",
    "        elif y.startswith(\"E-\"):\n",
    "            # Record end char position\n",
    "            end_char = x.token_features[i]['end_char']\n",
    "\n",
    "            # Add multi-token entity\n",
    "            ents['concept'].append(y[2:])\n",
    "            ents['phrase'].append(x.order_results[start_char:end_char])\n",
    "            ents['start_char'].append(start_char)\n",
    "            ents['end_char'].append(end_char)\n",
    "            \n",
    "            # Reset start_char, end_char (optional)\n",
    "            start_char, end_char = None, None\n",
    "\n",
    "    # Convert to dataframe    \n",
    "    tmp = pd.DataFrame(ents)\n",
    "    \n",
    "    # Add metadata\n",
    "    tmp['histopathology_id'] = x.histopathology_id\n",
    "    tmp['patient_id'] = x.patient_id\n",
    "    tmp['report_no'] = x.report_no\n",
    "    tmp['fold'] = x.fold   \n",
    "    \n",
    "    # Add to the table of detected concepts\n",
    "    detected_concepts = pd.concat([detected_concepts, tmp], axis=0, ignore_index=True)   \n",
    "    \n",
    "detected_concepts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c686ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_concepts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fa281e",
   "metadata": {},
   "source": [
    "### Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed66a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = get_feature_names('concepts', ('B-', 'I-', 'E-', 'S-'))\n",
    "evaluate_ner(df[['histopathology_id', 'fold']], true_concepts_bioes, detected_concepts_bioes, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a91fa36",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_names = get_feature_names('concepts')\n",
    "evaluate_ner(df[['histopathology_id', 'fold']], true_concepts, detected_concepts, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20527ae",
   "metadata": {},
   "source": [
    "### Plot comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e1692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = get_feature_names('concepts')\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edd1105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary-based precision\n",
    "dict_prec_mean = [0.92, 0.75, 0.82, 0.45, 0.94, 0.15, 0.04, 0.01, 0.14]\n",
    "dict_prec_std = [0.13, 0.1, 0.3, 0.41, 0.05, 0.03, 0.02, 0.02, 0.04]\n",
    "\n",
    "# Dictionary-based recall\n",
    "dict_rec_mean = [0.53, 0.93, 0.92, 0.60, 0.95, 0.86, 0.83, 0.58, 0.98]\n",
    "dict_rec_std = [0.35, 0.04, 0.15, 0.39, 0.09, 0.1, 0.17, 0.5, 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45e5ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame({'feature_names': feature_names, \n",
    "                       'dict_prec_mean': dict_prec_mean, \n",
    "                       'dict_prec_std': dict_prec_std, \n",
    "                       'crf_prec_mean': crf_prec_mean, \n",
    "                       'crf_prec_std': crf_prec_std, \n",
    "                       'dict_rec_mean': dict_rec_mean, \n",
    "                       'dict_rec_std': dict_rec_std, \n",
    "                       'crf_rec_mean': crf_rec_mean, \n",
    "                       'crf_rec_std': crf_rec_std})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc2912",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10, 4)\n",
    "\n",
    "colors = (sns.color_palette()[0], sns.color_palette()[3])\n",
    "\n",
    "# Dictionary-based approach\n",
    "plt.errorbar(x=scores.feature_names, y=scores.dict_prec_mean, yerr=scores.dict_prec_std, \n",
    "             fmt='o', capsize=2, color=colors[0], label=\"Dictionary\")\n",
    "\n",
    "# CRF\n",
    "plt.errorbar(x=scores.feature_names, y=scores.crf_prec_mean, yerr=scores.crf_prec_std, \n",
    "             fmt='o', capsize=2, color=colors[1], label=\"CRF\")\n",
    "\n",
    "plt.legend();\n",
    "plt.xticks(rotation=90);\n",
    "plt.title(\"Precision CV\");\n",
    "plt.ylim([-0.2, 1.5])\n",
    "\n",
    "plt.savefig(\"comparison_precision_cv\", dpi=300, bbox_inches=\"tight\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3204e7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10, 4)\n",
    "\n",
    "colors = (sns.color_palette()[0], sns.color_palette()[3])\n",
    "\n",
    "# Dictionary-based approach\n",
    "plt.errorbar(x=scores.feature_names, y=scores.dict_rec_mean, yerr=scores.dict_rec_std, \n",
    "             fmt='o', capsize=2, color=colors[0], label=\"Dictionary\")\n",
    "\n",
    "# CRF\n",
    "plt.errorbar(x=scores.feature_names, y=scores.crf_rec_mean, yerr=scores.crf_rec_std, \n",
    "             fmt='o', capsize=2, color=colors[1], label=\"CRF\")\n",
    "\n",
    "plt.legend();\n",
    "plt.xticks(rotation=90);\n",
    "plt.title(\"Recall CV\");\n",
    "plt.ylim([-0.2, 1.5]);\n",
    "\n",
    "plt.savefig(\"comparison_recall_cv\", dpi=300, bbox_inches=\"tight\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36a5b00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
